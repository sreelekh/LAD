{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T03:07:46.022714Z",
     "start_time": "2021-02-05T03:07:46.005382Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    filename, extension = os.path.splitext(file_path)\n",
    "    name=(os.path.basename(file_path))\n",
    "    if (extension=='.mat'):\n",
    "        try:\n",
    "            mat = scipy.io.loadmat(file_path)\n",
    "            df = pd.DataFrame(np.hstack((mat['X'], mat['y'])))\n",
    "            X,y=mat['X'], mat['y']\n",
    "            return df,X,y\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                arrays = {}\n",
    "                f = h5py.File(file_path)\n",
    "                for k, v in f.items():\n",
    "                    arrays[k] = np.array(v)\n",
    "                X,y=arrays['X'].T,arrays['y'].T\n",
    "                df = pd.DataFrame(np.hstack((arrays['X'].T, arrays['y'].T)))\n",
    "                return df,X,y\n",
    "            except:\n",
    "                print(\"1 Failed to load\", name)\n",
    "\n",
    "    elif extension=='.csv':\n",
    "        try:\n",
    "            df = pd.read_csv(file_path,low_memory=False,delimiter=',',header=None)\n",
    "            df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]\n",
    "            if df1.shape[1]==0:\n",
    "                df = pd.read_csv(file_path,low_memory=False,delimiter=',')\n",
    "                df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]                \n",
    "            X=np.array(df1).astype(float)\n",
    "            y=df.drop(df.columns[(df.dtypes=='float')+(df.dtypes=='int')],axis=1)\n",
    "            return df,X,y\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path,low_memory=False,delimiter=',')\n",
    "                df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]\n",
    "                X=np.array(df1).astype(float)\n",
    "                y=df.drop(df.columns[(df.dtypes=='float')+(df.dtypes=='int')],axis=1)\n",
    "                return df,X,y\n",
    "            except:\n",
    "                print(\"2 Failed to load\", name)\n",
    "\n",
    "    elif extension=='.pickle':\n",
    "        try:\n",
    "            data=pickle.load(open(file_path,'rb'))\n",
    "            X=data['X']\n",
    "            y=data['y']\n",
    "            return data,X,y\n",
    "        except:\n",
    "            try:\n",
    "                d = pickle.load( open(file_path, \"rb\" ) )\n",
    "                data=d['rawdata']\n",
    "                labels=d['labels']\n",
    "                X=np.array(data).astype(float)\n",
    "                y=labels\n",
    "                df=np.hstack((X,y))\n",
    "                return df,X,y\n",
    "            except:\n",
    "                print(\"3 Failed to load\", name)\n",
    "                \n",
    "    elif extension=='.arff':\n",
    "        data = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data[0])\n",
    "        df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]\n",
    "        X=np.array(df1).astype(float)\n",
    "        y=df.drop(df.columns[(df.dtypes=='float')+(df.dtypes=='int')],axis=1)\n",
    "        return df,X,y\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to load the extension\",extension)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T03:07:48.106900Z",
     "start_time": "2021-02-05T03:07:47.987862Z"
    },
    "code_folding": [
     154,
     208
    ]
   },
   "outputs": [],
   "source": [
    "def eval_perf_timed(X,y):\n",
    "        data=X\n",
    "        labels=y\n",
    "        outliers_fraction= np.sum(y)/len(y)\n",
    "        eval_metrics={}\n",
    "        eval_preds={}\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X_resampled, y_resampled = ros.fit_resample(np.arange(0,len(X),1).reshape(-1,1), y)\n",
    "        train, test = train_test_split(X_resampled, test_size=0.2,random_state=200)\n",
    "        train=train[:,0]\n",
    "        if 1:\n",
    "            eval_metrics['LOF']={}\n",
    "            eval_metrics['LOF']['fpr']={}\n",
    "            eval_metrics['LOF']['tpr']={}\n",
    "            eval_metrics['LOF']['thresholds']={}\n",
    "            eval_metrics['LOF']['fp']={}\n",
    "            eval_metrics['LOF']['tp']={}\n",
    "            eval_metrics['LOF']['fn']={}\n",
    "            eval_metrics['LOF']['tn']={}\n",
    "            eval_metrics['LOF']['recall']={}\n",
    "            eval_metrics['LOF']['specificity']={}\n",
    "            eval_metrics['LOF']['precision']={}\n",
    "            eval_metrics['LOF']['accuracy']={}\n",
    "            eval_metrics['LOF']['fmeasure']={}\n",
    "            eval_metrics['LOF']['purity']={}\n",
    "            eval_metrics['LOF']['auc']={}    \n",
    "            eval_metrics['LOF']['comp_time_secs']={}    \n",
    "\n",
    "\n",
    "            eval_metrics['Kmeans--']={}\n",
    "            eval_metrics['Kmeans--']['fpr']={}\n",
    "            eval_metrics['Kmeans--']['tpr']={}\n",
    "            eval_metrics['Kmeans--']['thresholds']={}\n",
    "            eval_metrics['Kmeans--']['fp']={}\n",
    "            eval_metrics['Kmeans--']['tp']={}\n",
    "            eval_metrics['Kmeans--']['fn']={}\n",
    "            eval_metrics['Kmeans--']['tn']={}\n",
    "            eval_metrics['Kmeans--']['recall']={}\n",
    "            eval_metrics['Kmeans--']['specificity']={}\n",
    "            eval_metrics['Kmeans--']['precision']={}\n",
    "            eval_metrics['Kmeans--']['accuracy']={}\n",
    "            eval_metrics['Kmeans--']['fmeasure']={}\n",
    "            eval_metrics['Kmeans--']['purity']={}\n",
    "            eval_metrics['Kmeans--']['auc']={}    \n",
    "            eval_metrics['Kmeans--']['comp_time_secs']={}    \n",
    "\n",
    "\n",
    "            eval_metrics['knn']={}\n",
    "            eval_metrics['knn']['fpr']={}\n",
    "            eval_metrics['knn']['tpr']={}\n",
    "            eval_metrics['knn']['thresholds']={}\n",
    "            eval_metrics['knn']['fp']={}\n",
    "            eval_metrics['knn']['tp']={}\n",
    "            eval_metrics['knn']['fn']={}\n",
    "            eval_metrics['knn']['tn']={}\n",
    "            eval_metrics['knn']['recall']={}\n",
    "            eval_metrics['knn']['specificity']={}\n",
    "            eval_metrics['knn']['precision']={}\n",
    "            eval_metrics['knn']['accuracy']={}\n",
    "            eval_metrics['knn']['fmeasure']={}\n",
    "            eval_metrics['knn']['purity']={}\n",
    "            eval_metrics['knn']['auc']={}    \n",
    "            eval_metrics['knn']['comp_time_secs']={}    \n",
    "\n",
    "\n",
    "            eval_metrics['ocsvm']={}\n",
    "            eval_metrics['ocsvm']['fpr']={}\n",
    "            eval_metrics['ocsvm']['tpr']={}\n",
    "            eval_metrics['ocsvm']['thresholds']={}\n",
    "            eval_metrics['ocsvm']['fp']={}\n",
    "            eval_metrics['ocsvm']['tp']={}\n",
    "            eval_metrics['ocsvm']['fn']={}\n",
    "            eval_metrics['ocsvm']['tn']={}\n",
    "            eval_metrics['ocsvm']['recall']={}\n",
    "            eval_metrics['ocsvm']['specificity']={}\n",
    "            eval_metrics['ocsvm']['precision']={}\n",
    "            eval_metrics['ocsvm']['accuracy']={}\n",
    "            eval_metrics['ocsvm']['fmeasure']={}\n",
    "            eval_metrics['ocsvm']['purity']={}\n",
    "            eval_metrics['ocsvm']['auc']={}    \n",
    "            eval_metrics['ocsvm']['comp_time_secs']={}    \n",
    "\n",
    "\n",
    "        # # try lof\n",
    "        i=0\n",
    "        with open('data'+str(i)+'.txt','w') as f:\n",
    "            f.write(\"%d %d\\n\"%(data.shape[0],data.shape[1]))\n",
    "            for i2 in range(data.shape[1]):\n",
    "                f.write(\"1 \")\n",
    "            f.write(\"\\n\")\n",
    "            for j in range(data.shape[0]): ##### Should add for multiple features\n",
    "                for i2 in range(data.shape[1]):\n",
    "                    f.write(\"%f \"%(data[j,i2]))\n",
    "                f.write(\"\\n\")\n",
    "        for nn in range(10,90,10):\n",
    "            start=time.time()\n",
    "            st = 'lof/main -test '+'data'+str(i)+'.txt'+' -o tmp'+str(i)+'.txt -n '+str(nn)+'  -m 1'\n",
    "            with open('cmdfile','w') as f:\n",
    "                f.write(st+'\\n')\n",
    "            !chmod +x cmdfile\n",
    "            !source cmdfile\n",
    "            pred = []\n",
    "            with open('tmp'+str(i)+'.txt_lof','r') as f:\n",
    "                for l in f:\n",
    "                    l = l.strip()\n",
    "                    pred.append(float(l))\n",
    "            end=time.time()\n",
    "\n",
    "            preds=np.zeros(len(np.array(pred)))\n",
    "            preds[np.argsort(-np.array(pred))[0:int(sum(y))]]=1\n",
    "            tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "            recall=tp/(tp+fn)\n",
    "            specificity=tn/(tn+fp)\n",
    "            precision=tp/(tp+fp)\n",
    "            accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "            fmeasure=2*precision*recall/(precision + recall)\n",
    "            purity=purity_score(y, preds)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "            auc=metrics.auc(fpr, tpr)\n",
    "            #print(\"\")\n",
    "            #print(\"LOF with n\",nn)\n",
    "            #print(\"\")\n",
    "            #print(\"Confusion matrix\",tn, fp, fn, tp)\n",
    "            #print(\"True Positives\",tp)\n",
    "            #print(\"Accuracy\",accuracy)\n",
    "            #print(\"Specificity\",specificity)\n",
    "            #print(\"Precision\",precision)\n",
    "            #print(\"Recall\",recall)\n",
    "            #print(\"F-measure\",fmeasure)\n",
    "            #print(\"Purity\",purity)\n",
    "            #print(\"AUC\",auc)\n",
    "            eval_preds['LOF']=preds\n",
    "            eval_metrics['LOF']['fpr'][nn]=fpr\n",
    "            eval_metrics['LOF']['tpr'][nn]=tpr\n",
    "            eval_metrics['LOF']['thresholds'][nn]=thresholds\n",
    "            eval_metrics['LOF']['fp'][nn]=fp\n",
    "            eval_metrics['LOF']['tp'][nn]=tp\n",
    "            eval_metrics['LOF']['fn'][nn]=fn\n",
    "            eval_metrics['LOF']['tn'][nn]=tn\n",
    "            eval_metrics['LOF']['recall'][nn]=recall\n",
    "            eval_metrics['LOF']['specificity'][nn]=specificity\n",
    "            eval_metrics['LOF']['precision'][nn]=precision\n",
    "            eval_metrics['LOF']['accuracy'][nn]=accuracy\n",
    "            eval_metrics['LOF']['fmeasure'][nn]=fmeasure\n",
    "            eval_metrics['LOF']['purity'][nn]=purity\n",
    "            eval_metrics['LOF']['auc'][nn]=auc\n",
    "            eval_metrics['LOF']['comp_time_secs'][nn]=end-start \n",
    "        print('LOF DONE')\n",
    "\n",
    "            \n",
    "        ks = [1,2,3,4,5,10,20,50,100]\n",
    "        #ks[1]\n",
    "#         data=X\n",
    "        for i in range(len(ks)):   \n",
    "            if ks[i]<len(data):\n",
    "                start=time.time()\n",
    "\n",
    "                linds, C, c = kmeans__(data,ks[i],int(np.sum(labels)))\n",
    "                preds = np.zeros([labels.shape[0],])\n",
    "                preds[linds] = 1\n",
    "                end=time.time()\n",
    "                \n",
    "\n",
    "                #print(\"\")\n",
    "                #print(\"Kmeans-- for \",ks[i])\n",
    "                #print(\"\")\n",
    "            #     #print( \"%.2f,%.2f\"%(precAtK(labels.astype(int),preds.astype(int)),averageRank(labels.astype(int),preds.astype(int))))\n",
    "                tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "                recall=tp/(tp+fn)\n",
    "                specificity=tn/(tn+fp)\n",
    "                precision=tp/(tp+fp)\n",
    "                accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "                fmeasure=2*precision*recall/(precision + recall)\n",
    "                fpr, tpr, thresholds = metrics.roc_curve(y, preds)\n",
    "                purity=purity_score(y, preds)\n",
    "                auc=metrics.auc(fpr, tpr)\n",
    "                #print(\"\")\n",
    "                #print(\"Confusion matrix\",tn, fp, fn, tp)\n",
    "                #print(\"True Positives\",tp)\n",
    "                #print(\"Accuracy\",accuracy)\n",
    "                #print(\"Specificity\",specificity)\n",
    "                #print(\"Precision\",precision)\n",
    "                #print(\"Recall\",recall)\n",
    "                #print(\"F-measure\",fmeasure)\n",
    "                #print(\"Purity\",purity)\n",
    "                #print(\"AUC\",auc)\n",
    "                eval_preds['Kmeans-- '+str(ks[i])]=preds\n",
    "                eval_metrics['Kmeans--']['fpr'][ks[i]]=fpr\n",
    "                eval_metrics['Kmeans--']['tpr'][ks[i]]=tpr\n",
    "                eval_metrics['Kmeans--']['thresholds'][ks[i]]=thresholds\n",
    "                eval_metrics['Kmeans--']['fp'][ks[i]]=fp\n",
    "                eval_metrics['Kmeans--']['tp'][ks[i]]=tp\n",
    "                eval_metrics['Kmeans--']['fn'][ks[i]]=fn\n",
    "                eval_metrics['Kmeans--']['tn'][ks[i]]=tn\n",
    "                eval_metrics['Kmeans--']['recall'][ks[i]]=recall\n",
    "                eval_metrics['Kmeans--']['specificity'][ks[i]]=specificity\n",
    "                eval_metrics['Kmeans--']['precision'][ks[i]]=precision\n",
    "                eval_metrics['Kmeans--']['accuracy'][ks[i]]=accuracy\n",
    "                eval_metrics['Kmeans--']['fmeasure'][ks[i]]=fmeasure\n",
    "                eval_metrics['Kmeans--']['purity'][ks[i]]=purity\n",
    "                eval_metrics['Kmeans--']['auc'][ks[i]]=auc\n",
    "                eval_metrics['Kmeans--']['comp_time_secs'][ks[i]]=end-start \n",
    "        print('Kmeans-- DONE')\n",
    "\n",
    "\n",
    "        ks = [1,2,3,4,5,10,20,50,100]\n",
    "        #ks[1]\n",
    "        for i in range(len(ks)):    \n",
    "            #try knn\n",
    "            start=time.time()\n",
    "            D = sp.distance.pdist(data)\n",
    "            D = sp.distance.squareform(D)\n",
    "            k = ks[i]\n",
    "            sD = np.sort(D,axis=1)\n",
    "            preds = np.mean(sD[:,0:k+1],axis=1)\n",
    "            end=time.time()\n",
    "\n",
    "            ##print \"%.2f,%.2f\"%(precAtK(labels.astype(int),preds.astype(int)),averageRank(labels.astype(int),preds.astype(int)))\n",
    "            pred = preds\n",
    "            #print(\"\")\n",
    "            #print(\"KNN for \",ks[i], \" neighbors\")\n",
    "            #print(\"\")\n",
    "        #     #print( \"%.2f,%.2f\"%(precAtK(labels.astype(int),preds.astype(int)),averageRank(labels.astype(int),preds.astype(int))))\n",
    "            preds=np.zeros(len(np.array(pred)))\n",
    "            preds[np.argsort(-np.array(pred))[0:int(sum(y))]]=1\n",
    "            tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "            recall=tp/(tp+fn)\n",
    "            specificity=tn/(tn+fp)\n",
    "            precision=tp/(tp+fp)\n",
    "            accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "            fmeasure=2*precision*recall/(precision + recall)\n",
    "            purity=purity_score(y, preds)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "            auc=metrics.auc(fpr, tpr)\n",
    "            #print(\"\")\n",
    "            #print(\"Confusion matrix\",tn, fp, fn, tp)\n",
    "            #print(\"True Positives\",tp)\n",
    "            #print(\"Accuracy\",accuracy)\n",
    "            #print(\"Specificity\",specificity)\n",
    "            #print(\"Precision\",precision)\n",
    "            #print(\"Recall\",recall)\n",
    "            #print(\"F-measure\",fmeasure)\n",
    "            #print(\"Purity\",purity)\n",
    "            #print(\"AUC\",auc)\n",
    "            eval_preds['knn '+str(ks[i])]=preds\n",
    "            eval_metrics['knn']['fpr'][ks[i]]=fpr\n",
    "            eval_metrics['knn']['tpr'][ks[i]]=tpr\n",
    "            eval_metrics['knn']['thresholds'][ks[i]]=thresholds\n",
    "            eval_metrics['knn']['fp'][ks[i]]=fp\n",
    "            eval_metrics['knn']['tp'][ks[i]]=tp\n",
    "            eval_metrics['knn']['fn'][ks[i]]=fn\n",
    "            eval_metrics['knn']['tn'][ks[i]]=tn\n",
    "            eval_metrics['knn']['recall'][ks[i]]=recall\n",
    "            eval_metrics['knn']['specificity'][ks[i]]=specificity\n",
    "            eval_metrics['knn']['precision'][ks[i]]=precision\n",
    "            eval_metrics['knn']['accuracy'][ks[i]]=accuracy\n",
    "            eval_metrics['knn']['fmeasure'][ks[i]]=fmeasure\n",
    "            eval_metrics['knn']['purity'][ks[i]]=purity\n",
    "            eval_metrics['knn']['auc'][ks[i]]=auc\n",
    "            eval_metrics['knn']['comp_time_secs'][ks[i]]=end-start \n",
    "        print('KNN DONE')\n",
    "\n",
    "\n",
    "    #     # try ocsvm\n",
    "        nu = float(np.sum(labels))/len(labels)\n",
    "        for gmma in np.arange(0.05,1,0.05):\n",
    "            ##print(gmma)\n",
    "            start=time.time()\n",
    "\n",
    "            oc = ocsvm(nu=nu,gamma=gmma)\n",
    "            oc.fit(X[train])\n",
    "            p = oc.predict(X)\n",
    "            preds = np.zeros(p.shape)\n",
    "            end=time.time()\n",
    "\n",
    "            preds[p == -1] = 1\n",
    "            preds[p == 1] = 0\n",
    "            #print(\"\")\n",
    "            #print(\"oc-SVM for gmma=\",gmma)\n",
    "            #print(\"\")\n",
    "        #     #print( \"%.2f,%.2f\"%(precAtK(labels.astype(int),preds.astype(int)),averageRank(labels.astype(int),preds.astype(int))))\n",
    "            tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "            recall=tp/(tp+fn)\n",
    "            specificity=tn/(tn+fp)\n",
    "            precision=tp/(tp+fp)\n",
    "            accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "            fmeasure=2*precision*recall/(precision + recall)\n",
    "            purity=purity_score(y, preds)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y, preds)\n",
    "            auc=metrics.auc(fpr, tpr)\n",
    "            #print(\"\")\n",
    "            #print(\"fpr\",fpr)\n",
    "            #print(\"Confusion matrix\",tn, fp, fn, tp)\n",
    "            #print(\"True Positives\",tp)\n",
    "            #print(\"Accuracy\",accuracy)\n",
    "            #print(\"Specificity\",specificity)\n",
    "            #print(\"Precision\",precision)\n",
    "            #print(\"Recall\",recall)\n",
    "            #print(\"F-measure\",fmeasure)\n",
    "            #print(\"Purity\",purity)\n",
    "            #print(\"AUC\",auc)\n",
    "            eval_preds['ocsvm '+str(gmma)]=preds\n",
    "            eval_metrics['ocsvm']['fpr'][gmma]=fpr\n",
    "            eval_metrics['ocsvm']['tpr'][gmma]=tpr\n",
    "            eval_metrics['ocsvm']['thresholds'][gmma]=thresholds\n",
    "            eval_metrics['ocsvm']['fp'][gmma]=fp\n",
    "            eval_metrics['ocsvm']['tp'][gmma]=tp\n",
    "            eval_metrics['ocsvm']['fn'][gmma]=fn\n",
    "            eval_metrics['ocsvm']['tn'][gmma]=tn\n",
    "            eval_metrics['ocsvm']['recall'][gmma]=recall\n",
    "            eval_metrics['ocsvm']['specificity'][gmma]=specificity\n",
    "            eval_metrics['ocsvm']['precision'][gmma]=precision\n",
    "            eval_metrics['ocsvm']['accuracy'][gmma]=accuracy\n",
    "            eval_metrics['ocsvm']['fmeasure'][gmma]=fmeasure\n",
    "            eval_metrics['ocsvm']['purity'][gmma]=purity\n",
    "            eval_metrics['ocsvm']['auc'][gmma]=auc\n",
    "            eval_metrics['ocsvm']['comp_time_secs'][gmma]=end-start \n",
    "        print('OCSVM DONE')\n",
    "\n",
    "        \n",
    "        return eval_metrics,eval_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T03:08:10.441339Z",
     "start_time": "2021-02-05T03:08:10.433571Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return parse(string, fuzzy=fuzzy).strftime(\"_%m_%d_%Y\")\n",
    "\n",
    "    except ValueError:\n",
    "        return (\"\")\n",
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=28,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width*3.1, row_height*2])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in six.iteritems(mpl_table._cells):\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_evaluate(X,y,preds):\n",
    "    tn, fp, fn, tp =confusion_matrix(y,preds).ravel().astype(float)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, preds, pos_label=2)\n",
    "    recall=tp/(tp+fn)\n",
    "    specificity=tn/(tn+fp)\n",
    "    precision=tp/(tp+fp)\n",
    "    accuracy=(tp+tn)/(tp+fp+tn+fn)\n",
    "    fmeasure=2*precision*recall/(precision + recall)\n",
    "    purity=purity_score(y, preds)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, preds)\n",
    "    auc=metrics.auc(fpr, tpr)\n",
    "    dic={}\n",
    "    dic['fpr']=fpr\n",
    "    dic['tpr']=tpr\n",
    "    dic['thresholds']=thresholds\n",
    "    dic['fp']=fp\n",
    "    dic['tp']=tp\n",
    "    dic['fn']=fn\n",
    "    dic['tn']=tn\n",
    "    dic['recall']=recall\n",
    "    dic['specificity']=specificity\n",
    "    dic['precision']=precision\n",
    "    dic['accuracy']=accuracy\n",
    "    dic['fmeasure']=fmeasure\n",
    "    dic['purity']=purity\n",
    "    dic['auc']=auc\n",
    "    return dic\n",
    "def ana_label(y):\n",
    "    labels,prop=np.unique(y, return_counts=True)\n",
    "    return (y==labels[np.argmin(prop)])*1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
