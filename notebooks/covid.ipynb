{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Libraries\n",
    "# import matplotlib\n",
    "# matplotlib.use('agg')\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
    "# %load_ext line_profiler\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time, random               # add some random sleep time\n",
    "import scipy\n",
    "import glob\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "import re\n",
    "import itertools\n",
    "import shutil\n",
    "import h5py\n",
    "import matplotlib\n",
    "from scipy.io import arff\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "# from spot import SPOT\n",
    "from os.path import isfile, join\n",
    "# from statsutils import *\n",
    "from boltons.statsutils import *\n",
    "from datetime import datetime\n",
    "from itertools import repeat,cycle, islice\n",
    "from ipywidgets import interact\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.utils import check_random_state, shuffle\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.svm import OneClassSVM as ocsvm\n",
    "from sklearn import cluster, datasets, metrics, mixture,svm\n",
    "from sklearn.neighbors import KNeighborsClassifier,LocalOutlierFactor,kneighbors_graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "from scipy import linalg\n",
    "from scipy.special import gamma, factorial, digamma,betaln, gammaln\n",
    "from scipy.stats import beta, multivariate_normal, wishart,invwishart,t, mode\n",
    "from scipy.stats import genextreme as gev\n",
    "import scipy.spatial as sp\n",
    "import scipy.io\n",
    "from scipy.io import arff\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib as mpl\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=2)\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "%matplotlib inline\n",
    "\n",
    "# Global Figure Parameters\n",
    "import matplotlib.pylab as pylab\n",
    "global label_size\n",
    "global fig_len\n",
    "global fig_wid\n",
    "global m_size\n",
    "global title_size\n",
    "\n",
    "fig_len=16\n",
    "fig_wid=16\n",
    "m_size=50\n",
    "title_size=50\n",
    "label_size=30\n",
    "\n",
    "plot_params = {'legend.fontsize': 60,\n",
    "          'figure.figsize': (fig_len, fig_wid),\n",
    "         'axes.labelsize': label_size,\n",
    "         'axes.titlesize':title_size,\n",
    "         'xtick.labelsize':label_size,\n",
    "         'ytick.labelsize':label_size}\n",
    "pylab.rcParams.update(plot_params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.core import *\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_covid_df(data_path):\n",
    "    data_source_covid = data_path\n",
    "    all_filenames = [i for i in glob.glob(data_source_covid+'/'+'*.{}'.format('csv'))]\n",
    "    data_deaths = pd.concat([pd.read_csv(f) for f in all_filenames if '_US' in f and 'death' in f])\n",
    "    data_confirmed = pd.concat([pd.read_csv(f) for f in all_filenames if '_US' in f and 'confirmed' in f])\n",
    "\n",
    "\n",
    "    data_deaths0=data_deaths.groupby(['FIPS', 'Admin2', 'Province_State','Country_Region']).sum().reset_index()\n",
    "    data_deaths1=data_deaths0[data_deaths0['Population']>50000]\n",
    "    data_deaths2=data_deaths1.drop(['UID',  'code3', 'FIPS', 'Admin2', 'Province_State','Country_Region', 'Lat', 'Long_'],axis=1)\n",
    "    data_deaths3=data_deaths2.div(data_deaths2['Population'],axis=0)\n",
    "    data_deaths3=data_deaths3.drop(['Population'],axis=1)\n",
    "\n",
    "\n",
    "    data_confirmed_0=data_confirmed.join(data_deaths[['FIPS','Province_State','Admin2','UID','Population']].set_index(['FIPS','Province_State','Admin2','UID']), on=['FIPS','Province_State','Admin2','UID'])\n",
    "    data_confirmed0=data_confirmed_0.groupby(['FIPS', 'Admin2', 'Province_State','Country_Region']).sum().reset_index()\n",
    "    data_confirmed1=data_confirmed0[data_confirmed0['Population']>50000]\n",
    "    data_confirmed2=data_confirmed1.drop(['UID',  'code3', 'FIPS', 'Admin2', 'Province_State','Country_Region', 'Lat', 'Long_'],axis=1)\n",
    "    data_confirmed3=data_confirmed2.div(data_confirmed2['Population'],axis=0)\n",
    "    data_confirmed3=data_confirmed3.drop(['Population'],axis=1)\n",
    "\n",
    "\n",
    "    data_deaths_full0=data_deaths.groupby(['FIPS', 'Admin2', 'Province_State','Country_Region']).sum().reset_index()\n",
    "    data_deaths_full1=data_deaths_full0[data_deaths_full0['Population']>50000]\n",
    "    data_deaths_full2=data_deaths_full1.drop(['UID',  'code3', 'FIPS', 'Admin2', 'Province_State','Country_Region', 'Lat', 'Long_'],axis=1)\n",
    "    data_deaths_full3=data_deaths_full2#.div(data_deaths_full2['Population'],axis=0)\n",
    "    data_deaths_full3=data_deaths_full3.drop(['Population'],axis=1)\n",
    "\n",
    "    data_confirmed_full_0=data_confirmed.join(data_deaths[['FIPS','Province_State','Admin2','UID','Population']].set_index(['FIPS','Province_State','Admin2','UID']), on=['FIPS','Province_State','Admin2','UID'])\n",
    "    data_confirmed_full0=data_confirmed_full_0.groupby(['FIPS', 'Admin2', 'Province_State','Country_Region']).sum().reset_index()\n",
    "    data_confirmed_full1=data_confirmed_full0[data_confirmed_full0['Population']>50000]\n",
    "    data_confirmed_full2=data_confirmed_full1.drop(['UID',  'code3', 'FIPS', 'Admin2', 'Province_State','Country_Region', 'Lat', 'Long_'],axis=1)\n",
    "    data_confirmed_full3=data_confirmed_full2#.div(data_confirmed_full2['Population'],axis=0)\n",
    "    data_confirmed_full3=data_confirmed_full3.drop(['Population'],axis=1)\n",
    "\n",
    "\n",
    "    data_deaths5=[]\n",
    "    N_counties,T =data_deaths3.shape\n",
    "    for i in range(N_counties):\n",
    "        dd=np.array(data_deaths3.iloc[i])\n",
    "        c_start=np.where(dd>0)[0]\n",
    "        if len(c_start)>0:\n",
    "            dd=np.append(dd[c_start[0]:], np.ones(c_start[0])*np.nan)\n",
    "        else:\n",
    "            dd=np.ones(T)*np.nan\n",
    "        data_deaths5.append(dd)\n",
    "    data_deaths5=pd.DataFrame(np.array(data_deaths5))\n",
    "    data_deaths5=data_deaths5.set_index(data_deaths3.index)\n",
    "\n",
    "\n",
    "    data_confirmed5=[]\n",
    "    N_counties,T =data_confirmed3.shape\n",
    "    for i in range(N_counties):\n",
    "        dd=np.array(data_confirmed3.iloc[i])\n",
    "        c_start=np.where(dd>0)[0]\n",
    "        if len(c_start)>0:\n",
    "            dd=np.append(dd[c_start[0]:], np.ones(c_start[0])*np.nan)\n",
    "        else:\n",
    "            dd=np.ones(T)*np.nan\n",
    "        data_confirmed5.append(dd)\n",
    "    data_confirmed5=pd.DataFrame(np.array(data_confirmed5))\n",
    "    data_confirmed5=data_confirmed5.set_index(data_confirmed3.index)\n",
    "\n",
    "    return data_deaths5, data_confirmed5, pd.DatetimeIndex(data_deaths3.columns)\n",
    "def load_data(file_path):\n",
    "    filename, extension = os.path.splitext(file_path)\n",
    "    name=(os.path.basename(file_path))\n",
    "    if (extension=='.mat'):\n",
    "        try:\n",
    "            mat = scipy.io.loadmat(file_path)\n",
    "            df = pd.DataFrame(np.hstack((mat['X'], mat['y'])))\n",
    "            X,y=mat['X'], mat['y']\n",
    "            return df,X,y\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                arrays = {}\n",
    "                f = h5py.File(file_path)\n",
    "                for k, v in f.items():\n",
    "                    arrays[k] = np.array(v)\n",
    "                X,y=arrays['X'].T,arrays['y'].T\n",
    "                df = pd.DataFrame(np.hstack((arrays['X'].T, arrays['y'].T)))\n",
    "                return df,X,y\n",
    "            except:\n",
    "                print(\"1 Failed to load\", name)\n",
    "\n",
    "    elif extension=='.csv':\n",
    "        try:\n",
    "            df = pd.read_csv(file_path,low_memory=False,delimiter=',',header=None)\n",
    "            df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]\n",
    "            if df1.shape[1]==0:\n",
    "                df = pd.read_csv(file_path,low_memory=False,delimiter=',')\n",
    "                df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]                \n",
    "            X=np.array(df1).astype(float)\n",
    "            y=df.drop(df.columns[(df.dtypes=='float')+(df.dtypes=='int')],axis=1)\n",
    "            return df,X,y\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path,low_memory=False,delimiter=',')\n",
    "                df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]\n",
    "                X=np.array(df1).astype(float)\n",
    "                y=df.drop(df.columns[(df.dtypes=='float')+(df.dtypes=='int')],axis=1)\n",
    "                return df,X,y\n",
    "            except:\n",
    "                print(\"2 Failed to load\", name)\n",
    "\n",
    "    elif extension=='.pickle':\n",
    "        try:\n",
    "            data=pickle.load(open(file_path,'rb'))\n",
    "            X=data['X']\n",
    "            y=data['y']\n",
    "            return data,X,y\n",
    "        except:\n",
    "            try:\n",
    "                d = pickle.load( open(file_path, \"rb\" ) )\n",
    "                data=d['rawdata']\n",
    "                labels=d['labels']\n",
    "                X=np.array(data).astype(float)\n",
    "                y=labels\n",
    "                df=np.hstack((X,y))\n",
    "                return df,X,y\n",
    "            except:\n",
    "                print(\"3 Failed to load\", name)\n",
    "                \n",
    "    elif extension=='.arff':\n",
    "        data = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data[0])\n",
    "        df1=df[df.columns[(df.dtypes=='float')+(df.dtypes=='int')]]\n",
    "        X=np.array(df1).astype(float)\n",
    "        y=df.drop(df.columns[(df.dtypes=='float')+(df.dtypes=='int')],axis=1)\n",
    "        return df,X,y\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to load the extension\",extension)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/lekhag/Documents/Testing Theories/COVID-19-master/csse_covid_19_data/csse_covid_19_time_series'\n",
    "deaths, confirmed, dates = cleaned_covid_df(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lad_z_initialization(X, K_f):\n",
    "    N,D = X.shape\n",
    "    if K_f>1:\n",
    "        if D>20:\n",
    "            pca = PCA(n_components=20).fit_transform(X)\n",
    "            X_train,  = train_test_split(pca, train_size=cluster_train)\n",
    "            kmeans = MiniBatchKMeans(n_clusters=K_f)\n",
    "            kmeans = kmeans.partial_fit(X_train)\n",
    "            return kmeans.predict(pca)+1\n",
    "\n",
    "        else:\n",
    "            X_train,  = train_test_split(X, train_size=cluster_train)\n",
    "            kmeans = MiniBatchKMeans(n_clusters=K_f)\n",
    "            kmeans = kmeans.partial_fit(X_train)\n",
    "            return kmeans.predict(X)+1\n",
    "    else:\n",
    "        return np.ones(N)        \n",
    "\n",
    "def cleaned_windowed_tsdb(X_full, history_time, start_time_step, d):\n",
    "    if history_time>0 and history_time<start_time_step:\n",
    "        X=np.copy(X_full[:,d-history_time:d])\n",
    "    else:\n",
    "        X=np.copy(X_full[:,:d])\n",
    "        \n",
    "    counties_missing=np.where(np.isnan(X).all(axis=1))[0]\n",
    "    counties_present=np.where(np.isnan(X).all(axis=1)*1==0)[0]\n",
    "\n",
    "    X=X[counties_present,:]\n",
    "    X[np.isnan(X)]=0\n",
    "    X[X==np.inf]=0\n",
    "\n",
    "    return X, counties_missing, counties_present\n",
    "\n",
    "def lad_entropy(X, K_f, K, z_1, cluster_log_probs, ana_score, th):\n",
    "    N_s,D_s=X.shape\n",
    "    entropy_2 = np.empty((N_s, K, D_s))\n",
    "    clusters,sizes=np.unique(np.abs(z_1),return_counts=True)\n",
    "    if len(clusters)<K_f:\n",
    "        cluster_prob=(np.exp(cluster_log_probs))+sys.float_info.min\n",
    "        cluster_prob/=np.sum(cluster_prob,axis=1)[:,np.newaxis]\n",
    "        z_1=np.array([np.argmax(np.random.multinomial(1,cluster_prob[i]))+1 for i in range(N)])\n",
    "    if len(np.unique(z_1))<K_f:\n",
    "        K_f=len(np.unique(z_1))\n",
    "\n",
    "    K=len(clusters)\n",
    "    thetas=[]\n",
    "    for k in clusters:\n",
    "        ind_k_0=z_1==k\n",
    "        ind_k=np.where(ind_k_0[ana_score<th])[0]\n",
    "        c = len(ind_k)\n",
    "        if c<3:\n",
    "            ind_k=np.abs(z_1)==k\n",
    "            thetas.append(tuple((np.mean(X[ind_k],axis=0), \n",
    "                        np.array([np.cov(X[:,d]) for d in range(D_s)]))))\n",
    "        else:\n",
    "            thetas.append(tuple((np.mean(X[ind_k],axis=0), \n",
    "                        np.array([np.cov(X[ind_k,d]) for d in range(D_s)]))))\n",
    "\n",
    "    cc=range(len(thetas))\n",
    "    means_=np.array([thetas[k][0].T for k in cc])\n",
    "    covariances=np.array([thetas[k][1] for k in cc])\n",
    "\n",
    "    ss=[1/(cov) for cov in covariances]\n",
    "    for k, (mu, prec_chol) in enumerate(zip(means_, ss)):\n",
    "        prec_chol[np.isnan(prec_chol)]=0\n",
    "        prec_chol[np.isinf(prec_chol)]=0\n",
    "        y=(np.square(X-mu)*prec_chol)/2\n",
    "        entropy_2[:, k, :] = (y)/sizes[k]\n",
    "\n",
    "    return entropy_2, sizes\n",
    "\n",
    "def updated_labels(entropy_DGProjection, z_1, sizes, D_s):\n",
    "    N_s, K_f = entropy_DGProjection.shape\n",
    "    if K_f>1:\n",
    "        cluster_log_probs=-(entropy_DGProjection*sizes)\n",
    "        z_1=np.argmax(cluster_log_probs,axis=1).flatten()+1\n",
    "    else:\n",
    "        cluster_log_probs = np.zeros((N_s, K))\n",
    "\n",
    "    ana_score=-np.max(-((entropy_DGProjection)), axis=1)\n",
    "    ana_score_0=np.copy(ana_score)\n",
    "    ana_score=(ana_score-np.min(ana_score))/(np.max(ana_score)-np.min(ana_score))\n",
    "    \n",
    "    th=min(0.95,np.mean(ana_score)+2*np.cov(ana_score))\n",
    "\n",
    "    z_1[ana_score>=th]*=(-1)\n",
    "    if np.sum(z_1<0)>min(0.25*N_s,D_s):\n",
    "        z_1[np.argsort(-ana_score)[:int(0.25*N_s)]]=-1*np.abs(z_1[np.argsort(-ana_score)[:int(0.25*N_s)]])\n",
    "        z_1[np.argsort(-ana_score)[int(0.25*N_s):]]=np.abs(z_1[np.argsort(-ana_score)[int(0.25*N_s):]])\n",
    "    \n",
    "    return z_1, ana_score_0, cluster_log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldp_ts_non_uniform(X_full,K_f,th_f,start_time_step,history_time,numiter):\n",
    "    N,D=X_full.shape\n",
    "    z={}\n",
    "    thresholds=np.zeros(D)\n",
    "    ana_score=np.zeros(N)\n",
    "    ana_prob_array=np.zeros((N,D))\n",
    "    K=np.copy(K_f)\n",
    "\n",
    "    z[start_time_step-1] = lad_z_initialization(X_full, K_f)\n",
    "    \n",
    "    for d in tqdm(np.arange(start_time_step,D)):\n",
    "        z_1=z[d-1]    \n",
    "\n",
    "        X, counties_missing, counties_present = cleaned_windowed_tsdb(X_full, history_time, start_time_step, d)\n",
    "\n",
    "        z_1=np.copy(z[d-1][counties_present])\n",
    "        N_s,D_s=X.shape\n",
    "        if N_s<1:\n",
    "            break\n",
    "\n",
    "        ana_score=np.zeros(N_s)\n",
    "        th=th_f\n",
    "\n",
    "        for iter in range(numiter):\n",
    "            # sys.stdout.write('.')\n",
    "            # print(d, iter)\n",
    "            if d==start_time_step and iter==0:\n",
    "                cluster_log_probs = np.zeros((N_s, K))\n",
    "            \n",
    "            entropy_2, sizes = lad_entropy(X, K_f, K, z_1, cluster_log_probs, ana_score, th)\n",
    "            \n",
    "            entropy_DGProjection=np.max(entropy_2,axis=2)\n",
    "            \n",
    "            z_1, ana_score_0, cluster_log_probs = updated_labels(entropy_DGProjection, z_1, sizes, D_s)\n",
    "        \n",
    "        ana_prob_array[counties_present,d]=np.copy(ana_score_0)\n",
    "        ana_prob_array[counties_missing,d]=np.nan\n",
    "        z[d]=np.zeros(N)\n",
    "        z[d][counties_present]=z_1\n",
    "        thresholds[d]=th\n",
    "        \n",
    "    ana_prob_array[np.isnan(ana_prob_array)]=0\n",
    "        \n",
    "    output={}\n",
    "    output['z']=z\n",
    "    output['ana_prob_array']=ana_prob_array\n",
    "    output['prob']=ana_score\n",
    "    output['entropy']=entropy_2\n",
    "    output['thresholds']=thresholds\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 613/691 [00:03<00:00, 194.66it/s]/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/lekhag/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 691/691 [00:03<00:00, 182.95it/s]\n"
     ]
    }
   ],
   "source": [
    "K=1\n",
    "reg_covar=1e-10\n",
    "covariance_type='full'\n",
    "th=0.5\n",
    "start_time_step=10\n",
    "numiter=10\n",
    "cluster_train=0.25\n",
    "\n",
    "# dates=pd.DatetimeIndex(data_deaths3.columns)\n",
    "\n",
    "# Results 1 cluster only\n",
    "X_full=np.array(deaths.fillna(0))\n",
    "history_time=1\n",
    "output=ldp_ts_non_uniform(X_full,K,th,start_time_step,history_time,numiter)\n",
    "# pickle.dump( output, open(image_path_covid+'Deaths_total_one_timestep'+\".pickle\", \"wb\" ))\n",
    "# plot_ldp_ts(361,output,image_path_covid,'Deaths_total_one_timestep','Deaths','Anomalous Trends in Total Deaths: Single Time Step',dates,data_deaths_full3,data_deaths3,data_deaths1)\n",
    "# print('d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bac469f55019de48f650e4127cb80381a21d8093ce4ae596eb1035e07145323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
