{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     20,
     34,
     53,
     81,
     83,
     144,
     151,
     195,
     242,
     287,
     301,
     335,
     362,
     383,
     399,
     406,
     444,
     461,
     467,
     488,
     509,
     535
    ]
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def plotClusters(thetas,z,samples):\n",
    "    thetameans = []\n",
    "    K = len(thetas)\n",
    "    plt.figure(figsize=(fig_len,fig_wid))\n",
    "    ax=plt.gca()\n",
    "    ax.set_facecolor('white')\n",
    "    ax.tick_params(labelsize=25)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.grid(color='k', linestyle='-.', linewidth=0.3)\n",
    "\n",
    "    for k in range(K):\n",
    "        thetameans.append(thetas[k][0])\n",
    "    thetameans = np.array(thetameans)\n",
    "    for k in range(K):\n",
    "        plt.scatter(samples[z == k,0],samples[z == k,1],marker='*',s=m_size)\n",
    "    plt.legend([str(k) for k in range(K)])\n",
    "    #plt.scatter(thetameans[:,0],thetameans[:,1],marker='x')\n",
    "    for k in range(K):\n",
    "        plt.text(thetameans[k,0],thetameans[k,1],str(k))\n",
    "def multivariatet(mu,Sigma,N,M):\n",
    "    '''\n",
    "    Output:\n",
    "    Produce M samples of d-dimensional multivariate t distribution\n",
    "    Input:\n",
    "    mu = mean (d dimensional numpy array or scalar)\n",
    "    Sigma = scale matrix (dxd numpy array)\n",
    "    N = degrees of freedom\n",
    "    M = # of samples to produce\n",
    "    '''\n",
    "    d = len(Sigma)\n",
    "    g = np.tile(np.random.gamma(N/2.,2./N,M),(d,1)).T\n",
    "    Z = np.random.multivariate_normal(np.zeros(d),Sigma,M)\n",
    "    return mu + Z/np.sqrt(g)\n",
    "def normalinvwishartsample(params):\n",
    "    '''\n",
    "    Generate sample from a Normal Inverse Wishart distribution\n",
    "\n",
    "    Inputs:\n",
    "    params - Parameters for the NIW distribution \n",
    "        mu    - Mean parameter: n x 1 numpy array\n",
    "        W     - Precision parameter: d x d numpy array\n",
    "        kappa - Scalar parameter for normal distribution covariance matrix\n",
    "        nu    - Scalar parameter for Wishart distribution\n",
    "\n",
    "    Output:\n",
    "    Sample - Sample mean vector, mu_s and Sample covariance matrix, W_s\n",
    "    '''\n",
    "    mu,W,kappa,nu = params\n",
    "    # first sample W from a Inverse Wishart distribution\n",
    "    W_s = invwishart(df=nu, scale=W).rvs()\n",
    "    mu_s = np.random.multivariate_normal(mu.flatten(),W_s/kappa,1) \n",
    "    return np.transpose(mu_s),W_s\n",
    "def normalinvwishartmarginal(X,params):\n",
    "    '''\n",
    "    Marginal likelihood of dataset X using a Normal Inverse Wishart prior\n",
    "\n",
    "    Inputs:\n",
    "    X      - Dataset matrix: n x d numpy array\n",
    "    params - Parameters for the NIW distribution \n",
    "        mu    - Mean parameter: n x 1 numpy array\n",
    "        W     - Precision parameter: d x d numpy array\n",
    "        kappa - Scalar parameter for normal distribution covariance matrix\n",
    "        nu    - Scalar parameter for Wishart distribution\n",
    "\n",
    "    Output:\n",
    "    Marginal likelihood of X - scalar\n",
    "    '''\n",
    "    mu,W,kappa,nu = params\n",
    "    mu=X\n",
    "\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    nu_n = nu + n\n",
    "    kappa_n = kappa + n\n",
    "    X_mean = np.mean(X,axis=0)\n",
    "    X_mean = X_mean[:,np.newaxis]\n",
    "    S = scatter(X)\n",
    "    W_n = W + S + ((kappa*n)/(kappa+n))*np.dot(mu - X_mean,np.transpose(mu - X_mean))\n",
    "    #(1/np.power(np.pi,n*d*0.5))*(gamma(nu_n*0.5)/gamma(nu*0.5))*(np.power(np.linalg.det(W),nu*0.5)/np.power(np.linalg.det(W_n),nu_n*0.5))*np.power(kappa/kappa_n,0.5*d)\n",
    "    return (1/np.power(np.pi,n*d*0.5))*(gamma(nu_n*0.5)/gamma(nu*0.5))*(np.power(np.linalg.det(W)/np.linalg.det(W_n),nu*0.5)/np.power(np.linalg.det(W_n),(nu_n-nu)*0.5))*np.power(kappa/kappa_n,0.5*d)\n",
    "def scatter(x):\n",
    "    return np.dot(np.transpose(x - np.mean(x,0)),x - np.mean(x,0))\n",
    "def plotAnomalies(I,samples):\n",
    "    for k in range(2):\n",
    "        plt.figure(figsize=(fig_len,fig_wid))\n",
    "        ax=plt.gca()\n",
    "        ax.set_facecolor('white')\n",
    "        ax.tick_params(labelsize=20)\n",
    "        ax.set_facecolor('white')\n",
    "        ax.grid(color='k', linestyle='-.', linewidth=0.3)\n",
    "        plt.scatter(samples[I == k,0],samples[I == k,1],marker='*',s=m_size)\n",
    "def kmeans__(data,k,l,maxiters=100,eps=0.0001):\n",
    "\n",
    "    # select k cluster centers\n",
    "    C = data[np.random.permutation(range(data.shape[0]))[0:k],:]\n",
    "    objVal = 0\n",
    "    for jj in range(maxiters):\n",
    "        # compute distance of each point to the clusters\n",
    "        dMat = pdist2(data,C)\n",
    "        d = np.min(dMat,axis=1).flatten()\n",
    "        c = np.argmin(dMat,axis=1).flatten()\n",
    "        # sort points by distance to their closest center\n",
    "        inds = np.argsort(d)[::-1]\n",
    "        linds = inds[0:l]\n",
    "        cinds = inds[l+1:]\n",
    "        # extract the non-outlier data objects\n",
    "        ci = c[cinds]\n",
    "        # recompute the means\n",
    "        for kk in range(k):\n",
    "            C[kk,:] = np.mean(data[np.where(ci == kk)[0],:],axis=0)\n",
    "        # compute objective function\n",
    "        objVal_ = objVal\n",
    "        objVal = 0\n",
    "        for kk in range(k):\n",
    "            objVal += np.sum(pdist2(data[np.where(ci == kk)[0],:],C[kk,:]))\n",
    "        if np.abs(objVal - objVal_) < eps:\n",
    "            break\n",
    "    # one final time\n",
    "    dMat = pdist2(data,C)\n",
    "    c = np.argmin(dMat,axis=1).flatten()\n",
    "    return linds, C, c\n",
    "def pdist2(X,C):\n",
    "    if len(C.shape) == 1:\n",
    "        C = C[:,np.newaxis]\n",
    "    distMat = np.zeros([X.shape[0],C.shape[0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(C.shape[0]):\n",
    "            distMat[i,j] = np.linalg.norm(X[i,:] - C[j,:])\n",
    "    return distMat\n",
    "def precAtK(true,predicted):\n",
    "    # find number of anomalies\n",
    "    k = np.sum(true)\n",
    "#     print(\"k=\",k)\n",
    "    # find the score of the k^th predicted anomaly\n",
    "    v = np.sort(predicted,axis=0)[::-1][k-1]\n",
    "#     print(\"v=\",v)\n",
    "    # find all objects that are above the threshold\n",
    "    inds = np.where(predicted >= v)[0]\n",
    "#     print(\"inds=\",inds)\n",
    "#     print(\"np.sum(true[inds])=\",np.sum(true[inds]))\n",
    "#     print(\"len(inds)=\",len(inds))\n",
    "#     print(\"np.sum(true[inds])/len(inds)=\",np.sum(true[inds])/len(inds))\n",
    "    return float(np.sum(true[inds]))/float(len(inds))\n",
    "def averageRank(true,predicted):\n",
    "    inds = np.where(true == 1)[0]\n",
    "    s = np.argsort(predicted)[::-1]\n",
    "    v = []\n",
    "    for ind in inds:\n",
    "        v.append(float(np.where(s == ind)[0]+1))\n",
    "    return np.mean(v)\n",
    "def purity_score(y_true, y_pred):\n",
    "    \"\"\"Purity score\n",
    "\n",
    "    To compute purity, each cluster is assigned to the class which is most frequent \n",
    "    in the cluster [1], and then the accuracy of this assignment is measured by counting \n",
    "    the number of correctly assigned documents and dividing by the number of documents.\n",
    "    We suppose here that the ground truth labels are integers, the same with the predicted clusters i.e\n",
    "    the clusters index.\n",
    "\n",
    "    Args:\n",
    "        y_true(np.ndarray): n*1 matrix Ground truth labels\n",
    "        y_pred(np.ndarray): n*1 matrix Predicted clusters\n",
    "    \n",
    "    Returns:\n",
    "        float: Purity score\n",
    "    \n",
    "    References:\n",
    "        [1] https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html\n",
    "    \"\"\"\n",
    "    # matrix which will hold the majority-voted labels\n",
    "    y_voted_labels = np.zeros(y_true.shape)\n",
    "    # Ordering labels\n",
    "    ## Labels might be missing e.g with set like 0,2 where 1 is missing\n",
    "    ## First find the unique labels, then map the labels to an ordered set\n",
    "    ## 0,2 should become 0,1\n",
    "    labels = np.unique(y_true)\n",
    "    ordered_labels = np.arange(labels.shape[0])\n",
    "    for k in range(labels.shape[0]):\n",
    "        y_true[y_true==labels[k]] = ordered_labels[k]\n",
    "    # Update unique labels\n",
    "    labels = np.unique(y_true)\n",
    "    # We set the number of bins to be n_classes+2 so that \n",
    "    # we count the actual occurence of classes between two consecutive bin\n",
    "    # the bigger being excluded [bin_i, bin_i+1[\n",
    "    bins = np.concatenate((labels, [np.max(labels)+1]), axis=0)\n",
    "\n",
    "    for cluster in np.unique(y_pred):\n",
    "        hist, _ = np.histogram(y_true[y_pred==cluster], bins=bins)\n",
    "        # Find the most present label in the cluster\n",
    "        winner = np.argmax(hist)\n",
    "        y_voted_labels[y_pred==cluster] = winner\n",
    "    \n",
    "    return accuracy_score(y_true, y_voted_labels)\n",
    "\n",
    "def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n",
    "    \"\"\"Estimate the log Gaussian probability.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "    precisions_chol : array-like\n",
    "        Cholesky decompositions of the precision matrices.\n",
    "        'full' : shape of (n_components, n_features, n_features)\n",
    "        'tied' : shape of (n_features, n_features)\n",
    "        'diag' : shape of (n_components, n_features)\n",
    "        'spherical' : shape of (n_components,)\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "    Returns\n",
    "    -------\n",
    "    log_prob : array, shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_components, _ = means.shape\n",
    "    # det(precision_chol) is half of det(precision)\n",
    "    log_det = _compute_log_det_cholesky(\n",
    "        precisions_chol, covariance_type, n_features)\n",
    "\n",
    "    if covariance_type == 'full':\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n",
    "            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == 'tied':\n",
    "        log_prob = np.empty((n_samples, n_components))\n",
    "        for k, mu in enumerate(means):\n",
    "            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)\n",
    "            log_prob[:, k] = np.sum(np.square(y), axis=1)\n",
    "\n",
    "    elif covariance_type == 'diag':\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (np.sum((means ** 2 * precisions), 1) -\n",
    "                    2. * np.dot(X, (means * precisions).T) +\n",
    "                    np.dot(X ** 2, precisions.T))\n",
    "\n",
    "    elif covariance_type == 'spherical':\n",
    "        precisions = precisions_chol ** 2\n",
    "        log_prob = (np.sum(means ** 2, 1) * precisions -\n",
    "                    2 * np.dot(X, means.T * precisions) +\n",
    "                    np.outer(row_norms(X, squared=True), precisions))\n",
    "    return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n",
    "def _compute_precision_cholesky(covariances, covariance_type):\n",
    "    \"\"\"Compute the Cholesky decomposition of the precisions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    covariances : array-like\n",
    "        The covariance matrix of the current components.\n",
    "        The shape depends of the covariance_type.\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "        The type of precision matrices.\n",
    "    Returns\n",
    "    -------\n",
    "    precisions_cholesky : array-like\n",
    "        The cholesky decomposition of sample precisions of the current\n",
    "        components. The shape depends of the covariance_type.\n",
    "    \"\"\"\n",
    "    estimate_precision_error_message = (\n",
    "        \"Fitting the mixture model failed because some components have \"\n",
    "        \"ill-defined empirical covariance (for instance caused by singleton \"\n",
    "        \"or collapsed samples). Try to decrease the number of components, \"\n",
    "        \"or increase reg_covar.\")\n",
    "\n",
    "    if covariance_type == 'full':\n",
    "        n_components, n_features, _ = covariances.shape\n",
    "        precisions_chol = np.empty((n_components, n_features, n_features))\n",
    "        for k, covariance in enumerate(covariances):\n",
    "            try:\n",
    "                cov_chol = linalg.cholesky(covariance, lower=True)\n",
    "            except linalg.LinAlgError:\n",
    "                raise ValueError(estimate_precision_error_message)\n",
    "            precisions_chol[k] = linalg.solve_triangular(cov_chol,\n",
    "                                                         np.eye(n_features),\n",
    "                                                         lower=True).T\n",
    "    elif covariance_type == 'tied':\n",
    "        _, n_features = covariances.shape\n",
    "        try:\n",
    "            cov_chol = linalg.cholesky(covariances, lower=True)\n",
    "        except linalg.LinAlgError:\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features),\n",
    "                                                  lower=True).T\n",
    "    else:\n",
    "        if np.any(np.less_equal(covariances, 0.0)):\n",
    "            raise ValueError(estimate_precision_error_message)\n",
    "        precisions_chol = 1. / np.sqrt(covariances)\n",
    "    return precisions_chol\n",
    "def _estimate_log_prob(means_,precisions_cholesky_,covariance_type,degrees_of_freedom_,mean_precision_, X):\n",
    "        _, n_features = X.shape\n",
    "        # We remove `n_features * np.log(degrees_of_freedom_)` because\n",
    "        # the precision matrix is normalized\n",
    "        log_gauss = (_estimate_log_gaussian_prob(\n",
    "            X, means_, precisions_cholesky_, covariance_type) -\n",
    "            .5 * n_features * np.log(degrees_of_freedom_))\n",
    "\n",
    "        log_lambda = n_features * np.log(2.) + np.sum(digamma(\n",
    "            .5 * (degrees_of_freedom_ -\n",
    "                  np.arange(0, n_features)[:, np.newaxis])), 0)\n",
    "\n",
    "        return log_gauss + .5 * (log_lambda -\n",
    "                                 n_features / mean_precision_)\n",
    "def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n",
    "    \"\"\"Compute the log-det of the cholesky decomposition of matrices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_chol : array-like\n",
    "        Cholesky decompositions of the matrices.\n",
    "        'full' : shape of (n_components, n_features, n_features)\n",
    "        'tied' : shape of (n_features, n_features)\n",
    "        'diag' : shape of (n_components, n_features)\n",
    "        'spherical' : shape of (n_components,)\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "    n_features : int\n",
    "        Number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    log_det_precision_chol : array-like, shape (n_components,)\n",
    "        The determinant of the precision matrix for each component.\n",
    "    \"\"\"\n",
    "    if covariance_type == 'full':\n",
    "        n_components, _, _ = matrix_chol.shape\n",
    "        log_det_chol = (np.sum(np.log(\n",
    "            matrix_chol.reshape(\n",
    "                n_components, -1)[:, ::n_features + 1]), 1))\n",
    "\n",
    "    elif covariance_type == 'tied':\n",
    "        log_det_chol = (np.sum(np.log(np.diag(matrix_chol))))\n",
    "\n",
    "    elif covariance_type == 'diag':\n",
    "        log_det_chol = (np.sum(np.log(matrix_chol), axis=1))\n",
    "\n",
    "    else:\n",
    "        log_det_chol = n_features * (np.log(matrix_chol))\n",
    "\n",
    "    return log_det_chol\n",
    "def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type):\n",
    "    \"\"\"Estimate the Gaussian distribution parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input data array.\n",
    "    resp : array-like, shape (n_samples, n_components)\n",
    "        The responsibilities for each data sample in X.\n",
    "    reg_covar : float\n",
    "        The regularization added to the diagonal of the covariance matrices.\n",
    "    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n",
    "        The type of precision matrices.\n",
    "    Returns\n",
    "    -------\n",
    "    nk : array-like, shape (n_components,)\n",
    "        The numbers of data samples in the current components.\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "        The centers of the current components.\n",
    "    covariances : array-like\n",
    "        The covariance matrix of the current components.\n",
    "        The shape depends of the covariance_type.\n",
    "    \"\"\"\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    covariances = {\"full\": _estimate_gaussian_covariances_full                   \n",
    "                  }[covariance_type](resp, X, nk, means, reg_covar)\n",
    "    return nk, means, covariances\n",
    "def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):\n",
    "    \"\"\"Estimate the full covariance matrices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    resp : array-like, shape (n_samples, n_components)\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "    nk : array-like, shape (n_components,)\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "    reg_covar : float\n",
    "    Returns\n",
    "    -------\n",
    "    covariances : array, shape (n_components, n_features, n_features)\n",
    "        The covariance matrix of the current components.\n",
    "    \"\"\"\n",
    "    n_components, n_features = means.shape\n",
    "    covariances = np.empty((n_components, n_features, n_features))\n",
    "    for k in range(n_components):\n",
    "        diff = X - means[k]\n",
    "        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n",
    "        covariances[k].flat[::n_features + 1] += reg_covar\n",
    "    return covariances\n",
    "def initialization(X,K,numiters,r0,alpha=1):\n",
    "    ########################### Data preprocessing\n",
    "    X,z,thetas,N,params,D=preprocess(X,K)\n",
    "\n",
    "    clusters, sizes = np.unique(z, return_counts=True)\n",
    "    m_para=sizes/N\n",
    "    F=np.zeros(N)\n",
    "#     params=tuple((np.array(pd.DataFrame(X).mean()),((np.array(pd.DataFrame(X).cov()))), 1, D))\n",
    "    \n",
    "#     m_para,F=F_est(np.ones,N,N,thetas,params,X)\n",
    "    \n",
    "    threshold=0.05\n",
    "    \n",
    "    I=(np.random.binomial(1, threshold, N))\n",
    "        \n",
    "    return X,z,I,thetas,N,params,D,clusters,sizes,m_para,F,threshold\n",
    "def convergence_check(thetas,centroids_old,conv_criteria):\n",
    "    centroids=np.copy(np.array(list([thetas[i][0] for i in range(len(thetas))])))\n",
    "    if len(centroids)<len(centroids_old):\n",
    "        change=np.sum(list(np.min(np.abs(np.linalg.norm(centroids_old-centroids[i],axis=1))) for i in range(len(centroids))))\n",
    "    else:\n",
    "        change=np.sum(list(np.min(np.abs(np.linalg.norm(centroids_old[i]-centroids,axis=1))) for i in range(len(centroids_old))))\n",
    "    return change\n",
    "def preprocess(X,K):\n",
    "    \n",
    "# Try different mean precision prior ie params[2] : No difference\n",
    "# mean_precision_prior float | None, optional.\n",
    "# The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. \n",
    "# Larger values concentrate the cluster means around mean_prior. The value of the parameter must be greater \n",
    "# than 0. If it is None, it is set to 1.\n",
    "\n",
    "# Try different reg_covar : Too volatile\n",
    "    if type(X) == list:\n",
    "        X = np.array(X)\n",
    "    if len(X.shape) == 1:\n",
    "        X = X[:,np.newaxis]\n",
    "    X=np.array(X).astype(float)\n",
    "\n",
    "    N = X.shape[0] #rows: observations\n",
    "    D = X.shape[1] #columns: dimensions\n",
    "\n",
    "    # Fit your data on the scaler object\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "#     X=normalize(X)\n",
    "    # Initialize z\n",
    "#     z=np.random.randint(K,size=N)\n",
    "    z=KMeans(K).fit(X).predict(X)\n",
    "    z+=1\n",
    "\n",
    "    if D>1:\n",
    "        params = tuple((np.mean(X,axis=0),(np.cov(X.T)), 1, D))\n",
    "    elif D==1:\n",
    "        params = tuple((np.mean(X),(np.var(X.T)),1, D))\n",
    "\n",
    "    z=np.random.randint(K,size=N)\n",
    "\n",
    "    thetas=[normalinvwishartsample(params) for k in range(K)]\n",
    "\n",
    "    return X,z,thetas,N,params,D\n",
    "def remove_cluster_new(X,z,K,thetas,params):\n",
    "#     if len(thetas)>len(np.unique(np.abs(z))):\n",
    "#         print(len(thetas)-len(np.unique(np.abs(z))),\" clusters removed\", len(np.unique(np.abs(z))) )\n",
    "    N=len(z)\n",
    "    z_temp=np.copy(z)\n",
    "    clusters, sizes = np.unique(np.abs(z_temp), return_counts=True)\n",
    "\n",
    "    c2=pd.DataFrame(clusters).copy()\n",
    "    temp=c2.index.copy()+1\n",
    "    c2.index=c2[0].copy()\n",
    "    c2[0]=temp.copy()\n",
    "    z=np.multiply(np.copy(c2[0][np.abs(z_temp)]),np.sign(z_temp+0.5))\n",
    "    \n",
    "    clusters, sizes = np.unique(np.abs(z), return_counts=True)\n",
    "    K=len(clusters)\n",
    "    \n",
    "    return z,K,thetas\n",
    "def compute_mixture_pdf(means_,precisions_cholesky_,covariance_type,mean_precision_, X,N,sizes):\n",
    "    degrees_of_freedom_=sizes+X.shape[1]\n",
    "    log_probs=_estimate_log_prob(means_,precisions_cholesky_,covariance_type,degrees_of_freedom_,mean_precision_, X)\n",
    "    MN=(np.exp(log_probs))\n",
    "    F=np.dot(sizes/N,np.exp(log_probs).T)\n",
    "    return degrees_of_freedom_,log_probs,MN,F\n",
    "def compute_cluster_params(z,X,params,clusters,sizes,ind_matrix,reg_covar,covariance_type):\n",
    "    K=(len(clusters))\n",
    "    N=len(z)\n",
    "    thetas=[]\n",
    "    for k in (clusters-1):\n",
    "        ind_k=np.where((z) == (k+1))[0]\n",
    "        c = len(ind_k)\n",
    "        if c<1:\n",
    "#             print(\"Group anomaly\")\n",
    "            ind_k=np.where(np.abs(z) == (k+1))[0]\n",
    "            c = len(ind_k)\n",
    "        thetas.append((_estimate_gaussian_parameters(X[ind_k], \n",
    "                                            np.ones((c,1)), reg_covar, covariance_type)[1:3]))    \n",
    "    nk=sizes\n",
    "    means_=np.array([thetas[k][0].T for k in clusters-1])[:,:,0]\n",
    "    covariances=np.array([thetas[k][1][0,:,:] for k in clusters-1])\n",
    "    para_tuple=nk,means_,covariances\n",
    "    precisions_cholesky_= np.array([_compute_precision_cholesky(cov, \n",
    "                                                covariance_type) for cov in [covariances]])[0,:,:,:]\n",
    "    \n",
    "    return para_tuple,thetas,nk,means_,covariances,precisions_cholesky_\n",
    "def _log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features):\n",
    "    \"\"\"Compute the log of the Wishart distribution normalization term.\n",
    "    Parameters\n",
    "    ----------\n",
    "    degrees_of_freedom : array-like, shape (n_components,)\n",
    "        The number of degrees of freedom on the covariance Wishart\n",
    "        distributions.\n",
    "    log_det_precision_chol : array-like, shape (n_components,)\n",
    "         The determinant of the precision matrix for each component.\n",
    "    n_features : int\n",
    "        The number of features.\n",
    "    Return\n",
    "    ------\n",
    "    log_wishart_norm : array-like, shape (n_components,)\n",
    "        The log normalization of the Wishart distribution.\n",
    "    \"\"\"\n",
    "    # To simplify the computation we have removed the np.log(np.pi) term\n",
    "    return -(degrees_of_freedom * log_det_precisions_chol +\n",
    "             degrees_of_freedom * n_features * .5 * math.log(2.) +\n",
    "             np.sum(gammaln(.5 * (degrees_of_freedom -\n",
    "                                  np.arange(n_features)[:, np.newaxis])), 0))\n",
    "def compute_log_likelihhod(z,sizes,K,precisions_cholesky_,covariance_type,features,degrees_of_freedom_,\n",
    "                           mean_precision_):\n",
    "        # Contrary to the original formula, we have done some simplification\n",
    "        # and removed all the constant terms.\n",
    "        log_resp=np.log(np.abs(z))\n",
    "        weight_concentration_ = (\n",
    "                1. + sizes,\n",
    "                (1/K +\n",
    "                 np.hstack((np.cumsum(sizes[::-1])[-2::-1], 0))))\n",
    "\n",
    "        # We removed `.5 * features * np.log(degrees_of_freedom_)`\n",
    "        # because the precision matrix is normalized.\n",
    "        log_det_precisions_chol = (_compute_log_det_cholesky(\n",
    "            precisions_cholesky_, covariance_type, features) -\n",
    "            .5 * features * np.log(degrees_of_freedom_))\n",
    "\n",
    "        log_wishart = np.sum(_log_wishart_norm(\n",
    "            degrees_of_freedom_, log_det_precisions_chol, features))\n",
    "        \n",
    "        log_norm_weight = -np.sum(betaln(weight_concentration_[0],\n",
    "                                         weight_concentration_[1]))\n",
    "\n",
    "        curr_log_likelihood=(-np.sum(np.exp(log_resp) * log_resp) -\n",
    "                log_wishart - log_norm_weight -\n",
    "                0.5 * features * np.sum(np.log(mean_precision_)))\n",
    "        return curr_log_likelihood\n",
    "def ppsa_vals(F,I,threshold):\n",
    "    N=len(F)\n",
    "    u=np.unique(F)\n",
    "    ps1=F\n",
    "    domain=((u>np.quantile(F,0.01))*1==(u<np.quantile(F,0.3))*1)\n",
    "    G_Y_domain=u*domain\n",
    "    G_Y_domain=(G_Y_domain[G_Y_domain>0])\n",
    "\n",
    "    G_Y=domain*[np.sum(F[(F<=u)]) for n,u in enumerate(np.unique(F))]\n",
    "    G_Y=G_Y[G_Y>0]\n",
    "    if len(G_Y)!=0:\n",
    "        G_Y=np.array(G_Y/max(G_Y))\n",
    "\n",
    "        g_Y=np.diff(G_Y)/np.diff(G_Y_domain)\n",
    "\n",
    "        aa=np.array(list(stats.percentileofscore(F, i)/100 for i in np.unique(F)))\n",
    "        th3=aa[aa<0.3][np.argmax(np.abs(np.diff(np.quantile(G_Y,aa[aa<0.3]))))]\n",
    "    else:\n",
    "        th3=threshold\n",
    "    len_tail=np.int(th3*N) #drop in F\n",
    "\n",
    "    u=np.quantile(ps1, th3)\n",
    "    inds = np.where(ps1<=u)[0]\n",
    "\n",
    "    iz=np.union1d(inds, np.where(I==1)[0])\n",
    "#     inds0=iz[np.argsort(ps1[iz])[:min(np.int(0.3*N),len(iz),np.int(th3*N))]]\n",
    "    inds0=np.argsort(ps1)[:min(np.int(0.3*N),len(iz),np.int(th3*N))]\n",
    "    psa = np.abs(ps1[inds0] - u) \n",
    "\n",
    "    gpdparams = stats.genpareto.fit(psa)\n",
    "    i_ppsa=np.zeros(N)\n",
    "    \n",
    "    i_ppsa[inds0] = stats.genpareto(1,0,gpdparams[2]).cdf(psa)\n",
    "    ppsa=np.ones(N)\n",
    "    \n",
    "    ppsa[inds0]=1-(i_ppsa[inds0])\n",
    "    \n",
    "#     i_ppsa[iz] = stats.genpareto(1,0,gpdparams[2]).cdf(np.abs(ps1[iz] - u))\n",
    "#     ppsa=np.ones(N)\n",
    "#     ppsa[iz]=1-(i_ppsa[iz])\n",
    "    \n",
    "    ppsa[ppsa>1]=1\n",
    "    ppsa[ppsa==0]=sys.float_info.min\n",
    "    return ppsa,i_ppsa, inds, inds0,u,F, threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
